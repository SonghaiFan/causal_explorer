{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def parse_cluster(file_path):\n",
    "    clusters = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        cluster_id = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if \"Cluster\" in line and \"- TOPIC:\" in line:\n",
    "                cluster_id = int(line.split()[1])\n",
    "                clusters[cluster_id] = {\n",
    "                    'topic': line.split(\"- TOPIC:\")[1].strip(),\n",
    "                    'content': []\n",
    "                }\n",
    "            elif line and cluster_id is not None:\n",
    "                item_number = line.split()[0][1:-1]\n",
    "                text = ' '.join(line.split()[1:])\n",
    "                clusters[cluster_id]['content'].append({\n",
    "                    'item_number': item_number,\n",
    "                    'text': text\n",
    "                })\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def parse_graph(file_path):\n",
    "    links = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if row['relation'] == 'C':\n",
    "                links.append(\n",
    "                    {'source': row['node_i'], 'target': row['node_j']})\n",
    "            elif row['relation'] == 'E':\n",
    "                links.append(\n",
    "                    {'source': row['node_j'], 'target': row['node_i']})\n",
    "    return links\n",
    "\n",
    "\n",
    "def create_json(cluster_path, graph_path, output_path):\n",
    "    clusters = parse_cluster(cluster_path)\n",
    "    links = parse_graph(graph_path)\n",
    "\n",
    "    data = {\n",
    "        'nodes': [{'id': str(k), 'topic': v['topic'], 'category': 'Default Behaviour', 'content': v['content']} for k, v in clusters.items()],\n",
    "        'links': links\n",
    "    }\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "\n",
    "# Call the function\n",
    "create_json(\"clusters_v2.txt\", \"graph_v2.csv\", \"data_v2.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file, save it as output_data variable\n",
    "with open('data_v2.json') as f:\n",
    "    output_data = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/songhaifan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/songhaifan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/songhaifan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "import random\n",
    "import json\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tableau_20 = ['#1f77b4', '#aec7e8', '#ff7f0e', '#ffbb78', '#2ca02c', '#98df8a', \n",
    "              '#d62728', '#ff9896', '#9467bd', '#c5b0d5', '#8c564b', '#c49c94', \n",
    "              '#e377c2', '#f7b6d2', '#7f7f7f', '#c7c7c7', '#bcbd22', '#dbdb8d', \n",
    "              '#17becf', '#9edae5']\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "def find_most_common_word(texts):\n",
    "    words = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['person', 'people', 'thing', 'things', 'another'])\n",
    "\n",
    "    for text in texts:\n",
    "        tokenized_text = word_tokenize(text)\n",
    "        words.extend(tokenized_text)\n",
    "\n",
    "    words = [word.lower() for word in words if word.isalpha() and word not in stop_words] \n",
    "    pos_words = pos_tag(words)\n",
    "\n",
    "    weighted_words = []\n",
    "    for word, pos in pos_words:\n",
    "        if pos.startswith('VB'):  # more weight to verbs\n",
    "            weighted_words.extend([word]*2)  # duplicate verb\n",
    "        else:\n",
    "            weighted_words.extend([word])\n",
    "\n",
    "    most_common_word = Counter(weighted_words).most_common(1)\n",
    "    return most_common_word[0][0] if most_common_word else None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_score(nodes, edges):\n",
    "    G = nx.Graph()\n",
    "    for node in nodes:\n",
    "        G.add_node(node['key'])\n",
    "\n",
    "    for edge in edges:\n",
    "        G.add_edge(*edge)\n",
    "\n",
    "    centrality = nx.betweenness_centrality(G)\n",
    "    for node in nodes:\n",
    "        node['score'] = centrality[node['key']]\n",
    "\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def calculate_layout(nodes, edges):\n",
    "    G = nx.Graph()\n",
    "    for node in nodes:\n",
    "        G.add_node(node['key'])\n",
    "\n",
    "    for edge in edges:\n",
    "        G.add_edge(*edge)\n",
    "\n",
    "    pos = nx.spring_layout(G, scale=2000, k=0.15)\n",
    "\n",
    "    for node in nodes:\n",
    "        node['x'], node['y'] = pos[node['key']]\n",
    "\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def json_to_dataset(json_data, default_cluster_color, default_tag_image):\n",
    "    # Prepare data structures\n",
    "    dataset = {\n",
    "        'nodes': [],\n",
    "        'edges': [],\n",
    "        'clusters': [],\n",
    "        'tags': [],\n",
    "        'labels': [],\n",
    "    }\n",
    "    clusters = {}\n",
    "    tags = set()\n",
    "    labels = set()\n",
    "\n",
    "    # Prepare color assignment for tags\n",
    "    tag_colors = {}\n",
    "\n",
    "    # Create nodes\n",
    "    for node in json_data['nodes']:\n",
    "        text_contents = [item['text'] for item in node['content']]\n",
    "        most_common_word = find_most_common_word(text_contents)\n",
    "        \n",
    "        # Assign a color from the palette based on the tag\n",
    "        tag_colors[node['category']] = tag_colors.get(node['category'], tableau_20[len(tag_colors) % len(tableau_20)])\n",
    "\n",
    "        dataset['nodes'].append({\n",
    "            'key': str(node['id']),\n",
    "            'label': most_common_word,\n",
    "            'tag': node['category'],  # Use category as tag\n",
    "            'URL': '',  # URL is not provided in the initial data\n",
    "            'cluster': node['id'],\n",
    "            'textContent': node['content'],\n",
    "        })\n",
    "\n",
    "        # Add a unique cluster for each node\n",
    "        dataset['clusters'].append({\n",
    "            'key': str(node['id']),\n",
    "            'color': tag_colors[node['category']],  # Cluster color depends on the tag\n",
    "            'clusterLabel': most_common_word,\n",
    "            'clusterTextContent': text_contents  # Add text contents to the cluster\n",
    "        })\n",
    "\n",
    "        clusters[str(node['id'])] = most_common_word\n",
    "        labels.add(most_common_word)\n",
    "        tags.add(node['category'])\n",
    "\n",
    "    # Create edges\n",
    "    for link in json_data['links']:\n",
    "        source = str(link['source'])\n",
    "        target = str(link['target'])\n",
    "        # check if the source and target nodes exist\n",
    "        if source in clusters and target in clusters:\n",
    "            dataset['edges'].append([source, target])\n",
    "\n",
    "    # Calculate layout\n",
    "    dataset['nodes'] = calculate_layout(dataset['nodes'], dataset['edges'])\n",
    "\n",
    "    # Create tags\n",
    "    for tag in tags:\n",
    "        dataset['tags'].append({\n",
    "            'key': tag,\n",
    "            'image': default_tag_image,  # Tag image is not provided in the initial data\n",
    "        })\n",
    "\n",
    "    # Create labels\n",
    "    for label in labels:\n",
    "        dataset['labels'].append({\n",
    "            'key': label,\n",
    "            'image': default_tag_image,\n",
    "        })\n",
    "\n",
    "    # Calculate scores\n",
    "    dataset['nodes'] = calculate_score(dataset['nodes'], dataset['edges'])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = json_to_dataset(output_data, '#6c3e81', 'unknown.svg')\n",
    "\n",
    "# save the dataset to a json file\n",
    "with open('../public/dataset.json', 'w') as f:\n",
    "    json.dump(dataset, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
